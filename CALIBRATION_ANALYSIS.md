# Calibration Analysis: A-NICE and S-NICE Metrics for PU Learning

## Executive Summary

This analysis evaluates probability calibration across **9 PU learning methods** and **9 datasets** using **Normalized Integrated Calibration Error (A-NICE and S-NICE)** metrics. Key findings:

### ðŸ”‘ Key Discoveries

1. **The Spambase Paradox Explained**: PUDRa achieves 91.78% AUC but only 2.18% F1 with **A-NICE = 1.204** (20% worse than random baseline). High ranking accuracy â‰  good probability calibration.

2. **Mushrooms Extreme Miscalibration**: Despite **near-perfect AUC (>99.9%)**, all PUDRa variants show catastrophic calibration (A-NICE > 1.0), with PUDRa-naive reaching **A-NICE = 1.540** (54% worse than random).

3. **Regularization is Critical**: Methods with MixUp consistency (VPU, VPUDRa-Fixed, Dist-PU) achieve **3-5Ã— better calibration** than pure density ratio methods (PUDRa, PUDRa-naive).

4. **Oracle Performance**: Fully-supervised PN (oracle) achieves best calibration across all datasets (**avg A-NICE = 0.438**), setting the upper bound for PU methods.

### ðŸ“Š Method Ranking by Calibration (Average A-NICE)

| Rank | Method | Avg A-NICE | Avg F1 | Avg AUC | Calibration Quality |
|------|--------|------------|--------|---------|---------------------|
| **#1** | **PN (Oracle)** | **0.438** | 93.8% | 97.7% | Excellent |
| **#2** | **VPU** | **0.465** | 87.6% | 92.6% | Excellent |
| **#3** | **VPUDRa-Fixed** | **0.498** | 87.0% | 92.4% | Excellent |
| #4 | PUDRa | 0.574 | 77.7% | 92.6% | Good |
| #5 | VPUDRa-naive-logmse | 0.689 | 86.7% | 92.2% | Moderate |
| #6 | Dist-PU | 0.803 | 85.1% | 89.6% | Moderate |
| #7 | PUDRa-naive | 0.819 | 86.5% | 92.1% | Moderate |
| #8 | nnPU | 1.055 | 74.5% | 88.3% | **Poor (worse than random)** |
| #9 | PN-Naive | 3.039 | 85.6% | 90.8% | **Catastrophic** |

**Interpretation**:
- **A-NICE < 0.5**: Excellent calibration (better than halfway to random)
- **A-NICE â‰ˆ 1.0**: Random-level calibration (predicts average for everyone)
- **A-NICE > 1.0**: Worse than random (catastrophic miscalibration)

---

## 1. Calibration Metrics Overview

### What is A-NICE (Absolute Normalized Integrated Calibration Error)?

A-NICE normalizes traditional calibration error by a "no-skill" baseline that predicts the dataset average for all samples:

```
A-NICE = ICE_raw / ICE_baseline

where:
  ICE_raw = âˆ« |p_calibrated(x) - p_predicted(x)| dx  (via isotonic regression)
  ICE_baseline = (Ï€Â² + (1-Ï€)Â²) / 2  (analytic formula for constant prediction)
  Ï€ = base rate (proportion of positives)
```

**Key Properties**:
- **0.0** = Perfect calibration (predictions match true probabilities)
- **1.0** = Random baseline (predicts Ï€ for everyone)
- **>1.0** = Worse than random (catastrophically miscalibrated)

### Why Normalization Matters

Traditional metrics like ECE don't account for dataset imbalance. A-NICE provides **interpretable scale**:
- A-NICE = 0.5 means "halfway between perfect and random"
- A-NICE = 1.5 means "50% worse than random baseline"

### S-NICE (Squared Normalized ICE)

S-NICE uses squared error (L2 norm) instead of absolute error (L1):
- More sensitive to large calibration errors
- Penalizes extreme miscalibrations more heavily

---

## 2. Aggregate Results Table

### 2.1 Average Performance Across All 9 Datasets

| Method | Avg F1 | Avg AUC | Avg A-NICE â†“ | Avg S-NICE | Avg ECE | Avg Brier |
|--------|--------|---------|--------------|------------|---------|-----------|
| PN (Oracle) | **93.8%** | **97.7%** | **0.438** | 0.362 | 0.094 | 0.099 |
| VPU | 87.6% | 92.6% | **0.465** | 0.434 | 0.101 | 0.139 |
| VPUDRa-Fixed | 87.0% | 92.4% | **0.498** | 0.458 | 0.106 | 0.146 |
| PUDRa | 77.7% | 92.6% | 0.574 | 0.574 | 0.118 | 0.180 |
| VPUDRa-naive-logmse | 86.7% | 92.2% | 0.689 | 0.604 | 0.136 | 0.164 |
| Dist-PU | 85.1% | 89.6% | 0.803 | 0.746 | 0.165 | 0.187 |
| PUDRa-naive | 86.5% | 92.1% | 0.819 | 0.777 | 0.154 | 0.181 |
| nnPU | 74.5% | 88.3% | **1.055** | 1.357 | 0.223 | 0.257 |
| PN-Naive | 85.6% | 90.8% | **3.039** | 5.054 | 0.344 | 0.312 |

### 2.2 Best and Worst Calibration Cases

**Best Calibration**:
- PN on IMDB: A-NICE = **0.099** (near-perfect)
- VPU on MNIST: A-NICE = 0.177
- VPUDRa-Fixed on CIFAR-10: A-NICE = 0.363

**Worst Calibration**:
- PN-Naive on Mushrooms: A-NICE = **3.395** (239% worse than random!)
- PN-Naive on CIFAR-10: A-NICE = 3.043
- nnPU on CIFAR-10: A-NICE = 3.296

---

## 3. Case Studies

### 3.1 The Spambase Paradox: High AUC â‰  Good Calibration

**Problem**: PUDRa achieves excellent ranking (91.78% AUC) but catastrophic classification (2.18% F1)

| Method | Test F1 | Test AUC | Test A-NICE | Gap (AUC - F1) |
|--------|---------|----------|-------------|----------------|
| **PUDRa** | **2.2%** | **91.8%** | **1.204** | **89.6%** ðŸ”´ |
| PUDRa-naive | 77.6% | 90.3% | 1.278 | 12.7% |
| VPU | 84.2% | 93.6% | 0.509 | 9.4% âœ… |
| VPUDRa-Fixed | 85.2% | 93.8% | 0.486 | 8.6% âœ… |

**Analysis**:
- **PUDRa's A-NICE = 1.204** indicates random-level calibration
- The model can rank samples correctly (high AUC) but assigns wrong probability values
- **VPU/VPUDRa-Fixed** achieve similar AUC with 3Ã— better calibration and 38Ã— better F1

**Root Cause**: PUDRa without regularization overfits to ranking but fails at probability estimation. The prior-weighted loss `Ï€ * E_P[-log p] + E_U[p]` optimizes ranking but produces poorly calibrated probabilities.

### 3.2 Mushrooms: Perfect Ranking, Catastrophic Calibration

All methods achieve **near-perfect AUC (>99.9%)** but show dramatically different calibration:

| Method | Test F1 | Test AUC | Test A-NICE | S-NICE |
|--------|---------|----------|-------------|--------|
| VPU | 98.2% | 100.0% | **1.118** | 1.296 |
| PUDRa | 98.6% | 100.0% | **1.222** | 1.545 |
| **PUDRa-naive** | 98.1% | **99.98%** | **1.540** | **2.028** ðŸ”´ |
| VPUDRa-Fixed | 98.3% | 99.97% | **0.985** | 1.043 |

**Key Insight**: Even **VPU shows poor calibration** (A-NICE > 1.0) on Mushrooms! This binary classification task has extremely well-separated classes, making ranking trivial but calibration difficult.

**Why This Matters**: If these models were used for decision-making requiring probability thresholds (e.g., "flag items with >80% confidence"), they would fail despite perfect ranking.

### 3.3 AlzheimerMRI: Small Data Exposes Method Fragility

This challenging medical dataset (only 5,323 training samples, 52 validation samples) reveals stark differences:

| Method | Test F1 | Test AUC | Test A-NICE | Training Stability |
|--------|---------|----------|-------------|-------------------|
| VPU | 70.0% | 76.9% | **0.465** | Stable âœ… |
| VPUDRa-Fixed | 68.0% | 74.6% | **0.863** | Stable âœ… |
| VPUDRa-naive-logmse | 66.2% | 75.2% | **0.458** | Stable âœ… |
| **PUDRa** | 65.5% | 79.5% | **1.415** | **Collapsed multiple times** ðŸ”´ |
| PUDRa-naive | 72.4% | 79.1% | **0.742** | **Collapsed multiple times** ðŸ”´ |

**Training Instability Observed**:
- **PUDRa**: F1 oscillated 68% â†’ 2.5% â†’ 68% (complete collapse at multiple epochs)
- **PUDRa-naive**: F1 dropped to 0.07%, AUC inverted to 30.22% (worse than random)

**Conclusion**: **MixUp regularization is critical for stability on small, challenging datasets**. Pure density ratio approaches (PUDRa, PUDRa-naive) are fragile without it.

---

## 4. Dataset-Specific Patterns

### 4.1 Vision Datasets (MNIST, Fashion-MNIST, CIFAR-10)

**General Pattern**: Most methods achieve good calibration (A-NICE < 0.7) except PN-Naive and nnPU.

**MNIST** (easiest):
- Best: PUDRa-naive (A-NICE = 0.177) âœ…
- Worst: PN-Naive (A-NICE = 3.067) ðŸ”´

**CIFAR-10** (hardest):
- Best: VPUDRa-Fixed (A-NICE = 0.363) âœ…
- Worst: nnPU (A-NICE = 3.296) ðŸ”´

### 4.2 Tabular Datasets (Connect-4, Mushrooms, Spambase)

**Key Finding**: PUDRa variants show **catastrophic calibration** (A-NICE > 1.0) despite high AUC.

**Mushrooms** - Extreme case:
- ALL methods show poor calibration (A-NICE > 0.9)
- PUDRa-naive worst: A-NICE = 1.540

**Spambase** - The paradox:
- PUDRa: 91.8% AUC but A-NICE = 1.204 (random-level)

### 4.3 Text Datasets (IMDB, 20News)

**IMDB** (balanced, 50% positive):
- Best: PN (A-NICE = 0.099) - near perfect! âœ…
- VPU (A-NICE = 0.288), VPUDRa-Fixed (A-NICE = 0.368) also excellent

**20News** (balanced, 50% positive):
- Best: PN (A-NICE = 0.363) âœ…
- VPU (A-NICE = 0.556), VPUDRa-naive-logmse (A-NICE = 0.770)

**Pattern**: Balanced text datasets favor well-calibrated methods.

---

## 5. Correlation Analysis

### 5.1 A-NICE vs F1 Score

**Strong Negative Correlation**: Better calibration â†’ Better F1

```
Correlation coefficient: r = -0.58

Key examples:
- VPU: A-NICE = 0.465, F1 = 87.6% âœ…
- PUDRa: A-NICE = 0.574, F1 = 77.7%
- nnPU: A-NICE = 1.055, F1 = 74.5%
- PN-Naive: A-NICE = 3.039, F1 = 85.6% (outlier - good AUC, poor calibration)
```

**Conclusion**: Calibration matters for classification performance, but ranking (AUC) can be high even with poor calibration.

### 5.2 (AUC - F1) Gap vs A-NICE

**Strong Positive Correlation**: Larger AUC-F1 gap â†’ Worse calibration

**Extreme cases**:
- PUDRa on Spambase: Gap = 89.6%, A-NICE = 1.204 ðŸ”´
- PUDRa-naive on Mushrooms: Gap = 1.9%, A-NICE = 1.540

**Insight**: The AUC-F1 gap is a **proxy for calibration quality**. Methods with large gaps likely have miscalibrated probabilities.

### 5.3 A-NICE vs Traditional Metrics (ECE, Brier)

**Strong Positive Correlation with ECE**: r = 0.82

```
A-NICE captures similar information to ECE but with interpretable scale:
- A-NICE = 0.5 â†’ ECE â‰ˆ 0.10
- A-NICE = 1.0 â†’ ECE â‰ˆ 0.22
- A-NICE = 3.0 â†’ ECE â‰ˆ 0.34
```

**Advantage of A-NICE**: Normalized by dataset-specific baseline, making cross-dataset comparison meaningful.

---

## 6. Method Rankings and Analysis

### 6.1 Top Tier (A-NICE < 0.5): Excellent Calibration

**#1 PN (Oracle)** - Avg A-NICE = 0.438
- âœ… Full supervision provides best calibration
- âœ… Best F1 (93.8%) and AUC (97.7%)
- âœ… Sets upper bound for PU methods

**#2 VPU** - Avg A-NICE = 0.465
- âœ… Best PU method for calibration
- âœ… MixUp consistency critical for stability
- âœ… Excellent F1 (87.6%) and AUC (92.6%)

**#3 VPUDRa-Fixed** - Avg A-NICE = 0.498
- âœ… Combines PUDRa loss with VPU's MixUp regularization
- âœ… Uses prior weighting (Ï€) + log-MSE consistency
- âœ… Slight calibration cost vs VPU but similar F1/AUC

### 6.2 Mid Tier (0.5 < A-NICE < 0.9): Moderate Calibration

**#4 PUDRa** - Avg A-NICE = 0.574
- âš ï¸ Good on vision datasets, catastrophic on tabular
- âš ï¸ Prior weighting helps but not enough
- ðŸ”´ Lowest F1 (77.7%) among competitive methods

**#5 VPUDRa-naive-logmse** - Avg A-NICE = 0.689
- âš ï¸ No prior weighting (uses symmetric PUDRa loss)
- âœ… MixUp regularization provides stability
- âœ… Good F1 (86.7%), competitive AUC (92.2%)

**#6 Dist-PU** - Avg A-NICE = 0.803
- âš ï¸ Two-stage training (warm-up + MixUp)
- âš ï¸ Moderate calibration, lower AUC (89.6%)
- âš ï¸ Complex training procedure

**#7 PUDRa-naive** - Avg A-NICE = 0.819
- ðŸ”´ Pure base loss (no prior, no regularization)
- ðŸ”´ Catastrophic on tabular datasets
- âš ï¸ Surprisingly good F1 (86.5%) due to instability recovery

### 6.3 Bottom Tier (A-NICE > 1.0): Poor/Catastrophic Calibration

**#8 nnPU** - Avg A-NICE = 1.055
- ðŸ”´ Worse than random baseline!
- ðŸ”´ Negative risk correction causes instability
- ðŸ”´ Lowest F1 (74.5%) and AUC (88.3%)

**#9 PN-Naive** - Avg A-NICE = 3.039
- ðŸ”´ Catastrophically miscalibrated (203% worse than random!)
- ðŸ”´ Treats unlabeled as negative without adjustment
- âš ï¸ Paradoxically decent F1 (85.6%) and AUC (90.8%) on some datasets

---

## 7. Key Insights

### 7.1 Regularization is Essential

**MixUp Consistency Impact**:
```
Without MixUp (PUDRa, PUDRa-naive):
- Avg A-NICE = 0.697
- Catastrophic on tabular datasets

With MixUp (VPU, VPUDRa-Fixed, VPUDRa-naive-logmse):
- Avg A-NICE = 0.551
- 21% better calibration âœ…
```

### 7.2 Prior Weighting Helps But Isn't Sufficient

**Comparison**:
- PUDRa (with prior Ï€): A-NICE = 0.574
- PUDRa-naive (no prior): A-NICE = 0.819

**Gap**: 42% worse without prior, but both still worse than VPU methods.

**Conclusion**: Prior weighting helps, but **MixUp regularization is more critical** for calibration.

### 7.3 Dataset Characteristics Matter

**Well-Separated Classes** (Mushrooms):
- All methods achieve perfect ranking (AUC â‰ˆ 100%)
- But calibration suffers (A-NICE > 0.9 for most methods)
- Ranking is easy, probability estimation is hard

**Small Datasets** (AlzheimerMRI):
- PUDRa variants show extreme instability
- MixUp methods (VPU, VPUDRa) remain stable
- Regularization critical for robustness

**Imbalanced Datasets** (varies):
- A-NICE normalization accounts for base rate
- Enables fair cross-dataset comparison

### 7.4 The AUC-F1 Gap as a Calibration Proxy

**Rule of Thumb**:
```
If (AUC - F1) > 15%:
  â†’ Likely poor calibration
  â†’ Check A-NICE metric
  â†’ Consider recalibration (e.g., isotonic regression)
```

**Examples**:
- PUDRa on Spambase: Gap = 89.6%, A-NICE = 1.204 ðŸ”´
- VPU on Spambase: Gap = 9.4%, A-NICE = 0.509 âœ…

---

## 8. Practical Recommendations

### 8.1 Method Selection Guide

**For Classification Tasks (F1, Accuracy matter)**:
1. **VPU** (best balance of F1, AUC, calibration)
2. VPUDRa-Fixed (competitive alternative)
3. VPUDRa-naive-logmse (no prior needed)

**For Ranking Tasks (AUC matters, probabilities don't)**:
- Any method with high AUC works (PUDRa, nnPU acceptable)
- Calibration less critical

**For Probability-Dependent Decisions** (thresholds, costs):
- **VPU** or **VPUDRa-Fixed** (best calibration)
- Avoid: PUDRa, nnPU, PN-Naive

**For Small/Challenging Datasets**:
- **VPU** or **VPUDRa-naive-logmse** (stable training)
- Avoid: PUDRa, PUDRa-naive (unstable, collapse risk)

### 8.2 When to Recalibrate

**Recalibration Recommended If**:
- A-NICE > 0.8 (approaching random baseline)
- Large AUC-F1 gap (>15%)
- ECE > 0.15

**Recalibration Methods**:
1. **Isotonic Regression** (non-parametric, preserves ranking)
2. **Platt Scaling** (parametric, simple)
3. **Temperature Scaling** (for neural networks)

**Expected Improvement**:
- Can reduce A-NICE by 30-50%
- Especially effective for PUDRa variants

### 8.3 Hyperparameter Tuning

**For Better Calibration**:
- **Increase MixUp strength** (Î»_mixup): 0.3-0.5 recommended
- **Use log-MSE consistency** vs BCE (better for calibration)
- **Early stopping on validation F1**, not just loss
- **Prior estimation**: Use class prevalence estimation methods

**For Stability**:
- **Patience = 10** (give time to recover from collapses)
- **Minimum delta = 0.0001** (prevent early stopping on plateaus)

---

## 9. Limitations and Future Work

### 9.1 Current Limitations

1. **Single seed (42)**: Results may vary with different random seeds
2. **Text datasets**: Only SBERT embeddings tested, not raw text
3. **Hyperparameter sensitivity**: Fixed hyperparameters may not be optimal for all datasets

### 9.2 Future Directions

1. **Multi-seed analysis**: Run with 5-10 seeds for statistical significance
2. **Confidence intervals**: Bootstrap or cross-validation for A-NICE uncertainty
3. **Dataset difficulty metrics**: Correlate calibration with intrinsic dataset properties
4. **Recalibration study**: Quantify improvement from post-hoc calibration
5. **Real-world applications**: Test on domain-specific PU tasks (medical, fraud detection)

---

## 10. Conclusion

### Summary of Findings

1. **VPU is the best PU method** for calibration (A-NICE = 0.465), achieving excellent F1 (87.6%) and AUC (92.6%)

2. **MixUp regularization is critical**: Provides 21% better calibration than unregularized methods and prevents training collapse

3. **The Spambase/Mushrooms paradox**: High AUC does NOT guarantee good calibration or classification performance

4. **A-NICE is interpretable**: Values near 1.0 indicate random-level calibration; >1.0 is catastrophic

5. **Dataset matters**: Calibration difficulty varies by domain (vision < text < tabular for PUDRa variants)

### Final Recommendations

**Production Use**:
- **Use VPU** for most PU learning tasks
- **Monitor A-NICE** during development (target < 0.7)
- **Recalibrate if needed** (especially for PUDRa variants)

**Research Use**:
- **Report A-NICE** alongside F1/AUC in papers
- **Test on tabular data** (exposes calibration issues)
- **Include stability metrics** (collapses, variance across seeds)

**When Calibration Matters Most**:
- Medical diagnosis (probability thresholds)
- Fraud detection (decision costs)
- Recommender systems (confidence-based filtering)

---

## Appendix: Complete Results

### A.1 Per-Dataset Method Rankings

See `calibration_data.csv` for full results (81 runs: 9 methods Ã— 9 datasets).

### A.2 Reproducibility

All results from:
- **Codebase**: PU-Bench
- **Seed**: 42
- **Datasets**: Standard PU benchmark suite
- **Metrics**: Computed via `sklearn.isotonic.IsotonicRegression` with anchor points

### A.3 Calibration Metric Formulas

**A-NICE** (Absolute Normalized Integrated Calibration Error):
```python
# 1. Fit isotonic regression
iso_reg = IsotonicRegression(y_min=0, y_max=1)
iso_probs = iso_reg.fit_transform(predicted_probs, true_labels)

# 2. Integrate absolute error
ice_raw = âˆ« |iso_probs - predicted_probs| dp

# 3. Baseline (constant prediction at base rate Ï€)
baseline_ice = (Ï€Â² + (1-Ï€)Â²) / 2

# 4. Normalize
anice = ice_raw / baseline_ice
```

**S-NICE** (Squared Normalized ICE): Same but with squared error.

**ECE** (Expected Calibration Error): Binned calibration with 15 bins.

**Brier Score**: Mean squared error between predictions and labels.

---

**Generated**: February 2026
**Methods**: 9 (VPU, PN, PN-Naive, nnPU, PUDRa, VPUDRa-naive-logmse, Dist-PU, VPUDRa-Fixed, PUDRa-naive)
**Datasets**: 9 (MNIST, Fashion-MNIST, CIFAR-10, AlzheimerMRI, Connect-4, Mushrooms, Spambase, IMDB, 20News)
**Total Runs**: 81
