cgenpu:
  # Training
  seed: 42
  num_epochs: 200
  batch_size: 32
  lr: 1.0e-4
  lr_d: 1.0e-4
  lr_a: 5.0e-5
  beta1: 0.5
  beta2: 0.999

  # Generator noise (reduced for text embeddings)
  latent_dim: 256
  z_type: "normal" # "uniform" | "normal"
  # Generator output activation for vector/text: tanh keeps range in [-1,1]
  gen_out_activation: tanh

  # Auxiliary classifier weight (increased for better alignment)
  aux_strength: 0.5
  # Pull A(x_u) mean toward class prior Ï€ to avoid all-positive collapse (ADNI helpful)
  prior_reg: 3.0
  # Extra prior alignment on real U (not only generated): stronger on ADNI
  prior_reg_u: 5.0
  # Smooth positive target for A to mitigate collapse
  pos_label_smoothing: 0.1
  # Freeze G for a few epochs to stabilize D/A first
  freeze_g_epochs: 5
  # Encourage generator to produce harder negatives earlier
  lr_g: 3.0e-4
  # Disable noise first to stabilize AUC, re-enable later if needed
  adni_aug_flip_prob: 0.25
  adni_aug_noise_std: 0.02

  # KL divergence reduction method (true for mean, false for sum)
  # Matches original TF implementation's 'mean' parameter
  reduce_mean: true

  # Checkpoint
  checkpoint:
    enabled: true
    save_model: false
    monitor: "val_f1"
    mode: "max"
    early_stopping:
      enabled: true
      patience: 30
      min_delta: 0.0001

  # Label scheme for PU datasets (stay consistent with others)
  label_scheme:
    true_positive_label: 1
    true_negative_label: 0
    pu_labeled_label: 1
    pu_unlabeled_label: -1
