vpudra_naive_logmse:
  # Optimizer hyperparameters (same as other VPUDRa variants)
  optimizer: "adam"
  lr: 0.0003
  weight_decay: 0.0001

  # Training schedule
  batch_size: 256
  num_epochs: 40
  seed: 42

  # VPUDRa-naive-logmse specific
  mix_alpha: 0.3      # Beta distribution parameter for MixUp (same as VPU)
  epsilon: 1e-7       # Numerical stability in log operations

  # Note: This variant uses the original PUDRa loss WITHOUT prior weighting
  #       + VPU's log-MSE consistency (NOT Point Process)
  #
  # Loss formulation:
  #   Base loss: E_P[-log p + p] + E_U[p]  (NO π multiplication!)
  #   Consistency: (log(μ_anchor) - log(p_mix))²  (VPU's log-MSE, symmetric)
  #
  # Comparison Matrix:
  #   |               | Log-MSE consistency    | Point Process consistency |
  #   |---------------|------------------------|---------------------------|
  #   | With prior    | VPUDRa-Fixed          | VPUDRa-PP                |
  #   | Without prior | VPUDRa-naive-logmse ← | VPUDRa-naive             |
  #
  # This tests whether:
  #   1. VPU's log-MSE consistency works better than Point Process when no prior
  #   2. The combination of symmetric base loss + symmetric consistency is optimal
  #
  # Key features:
  #   - Anchor assumption (for stability): μ = λ*p(x) + (1-λ)*1.0
  #   - VPU's log-MSE consistency: (log μ - log p)²
  #   - Original PUDRa base loss (no prior)
  #   - Most similar to pure VPU (same consistency loss)

  # Label scheme (standard PU convention)
  label_scheme:
    true_positive_label: 1
    true_negative_label: 0
    pu_labeled_label: 1
    pu_unlabeled_label: -1

  # Early stopping configuration
  checkpoint:
    enabled: true
    save_model: false
    monitor: "val_f1"      # Monitor validation F1 score
    mode: "max"            # Maximize F1
    early_stopping:
      enabled: true
      patience: 10         # Stop if no improvement for 10 epochs
      min_delta: 0.0001    # Minimum improvement threshold
